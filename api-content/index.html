{"posts":[{"title":"死锁的定义、产生原因、必要条件和处理方法","content":"死锁的定义 在一组进程发生死锁的情况下，这组死锁进程中的每个进程，都在等待另一个死锁进程所占有的资源。死锁的定义如下： 如果一个进程中的每一个进程都在等待仅该组进程中的其他进程才能引发的事件，那么该组进程是死锁的。 死锁的产生原因 死锁的起因，通常是源于多个进程对资源的争夺，不仅对不可抢占资源进行争夺或引起死锁，而且对可消耗资源进行争夺也会引起死锁。总结如下： 1、系统资源不足； 2、进程运行推进的顺序不当； 3、资源分配不当； 死锁产生的必要条件 1、互斥条件：进程在运行中对资源进行排他性使用，即一个资源仅能被一个进程使用，此时其他进程请求资源时，只能等待其释放。 2、请求与保持条件：某进程已经保持了一个资源，但又请求另一个资源，若该资源被其他进程占有，此时请求阻塞，且对已经占有的资源不释放； 3、不可抢占条件：进程获得的资源在未使用完时不可被抢占，只能在进程使用完时自己释放； 4、循环等待条件：发生死锁时，必然存在这样一个循环，一个进程p1等待p2占有的资源进程p2等待p3占有的资源...进程pn等待p1占有的资源。 死锁的处理方法 1、预防死锁：事先预防策略，容易实现，通过实现设置限制，破坏产生死锁的四个条件之一。(如对资源采用按序分配策略) 2、避免死锁：事先预防策略，在资源的动态分配过程中，用某些方法防止系统禁图不安全状态。常见的方法有银行家算法。 3、检测死锁：通过检测机构等及时检测出死锁，采取适当措施，把进程从死锁中解脱。 4、解除死锁：检测出死锁后，采取措施解决。比如剥夺资源，撤销进程。 这四种方法对死锁的防范逐渐减弱，但对应的是资源利用率的提高。 ","link":"https://devolner.github.io/I4n68tlsP/"},{"title":"c语言的qsort函数用法","content":"#include&lt;stdio.h&gt; #include&lt;stdlib.h&gt; 1.参数 qsort函数包含四个参数，分别是： 1.数组名 2.元素个数（从前往后计算） 3.数组元素所占字节（int，double，char等所占字节） 4.排序原则（递增，递减，奇偶交叉等） int cmp(const void a,const void b) { return (int)a-(int)b; } qsort(time,n,sizeof(int),cmp); ","link":"https://devolner.github.io/YNBGz08wZ/"},{"title":"大数据分析案例笔记版","content":"1.1 数据湖中存着所有的相关数据，历史的、实时的，在线的、离线的，内部的、外部的，结构化的、非结构化的，都能完整保存下来，方便“沙中淘金”。数据湖就应然而生，数据仓库里面放的都是有价值的，数据湖的是所有产生的数据，对未来的发展可能会产生的有价值的数据，这些数据就需要被治理，后来会数据产生智能化和自动化。 什么样的人可以拿到什么数据，因此要对数据进行分区， 1.2 什么样的人可以拿到什么数据，因此要对数据进行分区， 2.2数据&quot;淘金&quot;，寻找有用的数据，元数据存着的是结构， 湖仓一体化 将数据湖分出来一部分，作为数据仓库，让“数仓”在进行数据分析的时候，可以直接访问数据湖里的数据 2.1 数据仓库的特点及作用 效率足够高 数据质量 扩展性 面向主题 数据仓库架构的演变 离线，lambda--一套离线一套实时，kappa架构--一个系统可以离线可以实时（流批一体化） 数据来源： 日志系统，业务系统，爬虫系统 kafka主要做缓冲作用，日志作为文件用flume做 数据仓库是分层设计 ods--dwd--dws--ads 原数据---清洗过滤---聚合---应用层做统计分析 使用ads层的数据做数据报表/用户画像/推荐系统/机器学习 备份。清洗，聚合，统计 每一次都进行备份，为了数据恢复，回滚，和增加新的业务 ","link":"https://devolner.github.io/X2umF83DY/"},{"title":"服务器使用技巧","content":"**nohup java -jar meigui-0.0.1-SNAPSHOT.jar&gt;/logs/webapps.log --server.port=8888 2&gt;&amp;1 &amp; ** ","link":"https://devolner.github.io/X31_G6xD_/"},{"title":"spark笔记版","content":"flatMap一对多的算子，.map((,1))是变成二元组的形式。.reduceByKey(+).collect :quit 转换算子：flatMap，.map，.reduceByKey zookeeper实现高可用， 行动算子：collect，count，first，map，filter spark程序是懒执行，采用惰性计算，只有在使用的时候才会真正计算 stage，阶段 4040端口是有向无关图的画法 启动spark-shell创建了一个diriver，完成一个任务：提交，执行 sc代表了spark-context，sparkdriver提供了一个操作对象， rdd---spark dataframe---sparksql spark通过移动算子，但是因为也需要聚合，所以我们引入stage，引入了阶段，遇到一个阶段，我们就需要全部计算完成之后，再进行下一步计算，对于迭代式计算，用hadoop去处理就比较慢，spark就没有这个问题。 RDD的特性，一经创建就不可变，可分区（可将一个集合分区为一块一块的），里面的元素可以并行计算---因为可分区。 RDD操作----创建，转化，输出，RDD包含python，Java，scala，r语言的对象 自动容错---血统关系，记录血统链，错了可以根据它的“父母”创建。 累加器，广播变量。 分桶分的是文件，分区是分的目录， 弹性：自动进行内存和磁盘的切换，基于血统关系的高校容错机制，任务失败后会自动失败（默认重试4次），数据分区的高度弹性（随时可以变更分区数，在任务提交之后也可以，相较于hadoop） 依赖：出度好吧？？窄依赖：一对一；宽依赖：多对多 缓存：多次使用一个RDD，就缓存起来。 checkpoint：持久化存储 一个driver，提供主main执行各种操作，无论是spark-shell，pyspark，都是使用shell的外壳 flink基于流处理，dataStream，APIsql，可以学习参考快速入门案例，spark学完之后学一下flink可以吗，ddd sc.makeRDD(1 to 5);结果的collect;为执行算子，执行了它才能真正执行了make的操作，基于懒执行的原理 saprk.按tab，就有提示算子read.scv/json/.....options是可自定义的，spark最核心的是spark-sql，因为成本最低，学习最简单，高可用，高性能这些都差不多 2.3.2RDD的转换 只要知道是需要把什么类型转化成什么数据，res.flatMap(e=&gt;e.split(&quot; &quot;)).collect map,返回一个新的RDD，将类型进行转化为res.map(e=&gt;(e,1)) reduceByKey,//v,v=&gt;v,res.reduceByKey((v1,v2)=&gt;v1+v2),可以将相同的单词进行次数相加，聚合，可以简写为(下划线_+下划线_) filter，做一个过滤，sc.makeRDD(1 to 10).filter(e=&gt;e%2==0).collect 5.mapPartitions类似于map，作优化跟性能有关，2.6.2.4 mapPartitions(func) 类似于map，但独立地在RDD的每一个分区上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。对于一个分区中的所有数据执行一个函数，性能比map要高。文件一个g，但并不需要那么大的内存，因为迭代的时候会存入，也会移出 mapPartitionsSithIndex,更好用， sample随机抽样，抽取数据格式，转化算子 kakeSample行动算子/action算子 union求并集， intersection，求交集，感受不到分布式，算法还是按照单个集群的方式进行 distinct，它的顺序会发生变化，没有任何的排序操作，还有就是证明它确实采用了分布式导致结果不唯一 combineByKey类似于combine rdd1.zip(rdd2).cpllect//需要具备相同数量的key‘value,左边一个，右边一个。。。这样重复 (1,10)),(2,11).... 2.3.3行动算子 任务提交执行的算子， 1.reduce，将元素进行聚合rdd.reduce(_+__)/rdd.reduce((v1,v2)=&gt;v1+v2) count,take(1/n),takeOrdered(n)/先排序后拿走， saveAsTextFile(path)//配置过hadoop的config会保存到hdfs上，否则在磁盘上 序列化是内存到磁盘，反序列化就是加载到内存中 countByKey()//统计key的个数 foreach(func) 2.3.4统计分析的算子 数值型RDD的基本统计 2.3.5RDD的核心shuffle shuffle是重新对数据进行分区的操作，all-to-all，支持并发， 跨分区的操作，在需要跨分区才能完成任务例如所有的含有bykey需要执行shuffle操作，如 repartition 和 coalesce，ByKey操作（计数除外），如groupByKey 和 reduceByKey，以及连接操作，如cogroup和join。shuffle类似于一个hadoop中的一个mapreduce，但和spark的mr没有任何关系 宽依赖：stage划分，有shuffle 窄依赖：无stage划分，无shuffle，shuffle取决于转化算子， 根据出度不小于2决定，多个子RDD的Partition会依赖于一个RDD的partition，会引起shuffle， 只要一个RDD的partition的出度大于&gt;=2就是一个宽依赖 宽依赖可能会导致RDD的祖先都丢失，需要重新计算，so尽量使用窄依赖， 2.3.6DAG 血统链，有向无关图是无环的，不然计算的时候会进入死循环结构， 就是一个记录数据产生的过程，一次一次的转化之后的过程。 一个 jar 包就是一个 Application，就是一个driver，可以有多个行动操作/多个job 一个行动操作就是一个 Job ， 对应于 Hadoop 中的一个 MapReduce 任务 一个 Job 由很多 Stage 组成，划分 Stage 是从后往前划分，遇到宽依赖则将前面的所有转换划分为一个 Stage 一个 Stage 由很多Task组成，一个分区被一个 Task 所处理，所以分区数也叫并行度。 一个executor(线程）可以有多个task（进程），但一个task必须占用一个executor的核心,--executor-cores的核心数 任务执行的时候就会有缓存机制，可以设置persist，cache，cache是一个特殊的persist，可以设置缓存级别 服务器配置，ngnix spark.hnumi.com 检查点机制：CheckPoint,通过将RDD保存到一个非易失的存储，hdfs，高容错 通过检查点机制，它的血统关系会被切断，因为不会记录血统关系，就会产生断层，找不回原数据 3.1.1mysql数据源 val rdd = new JdbcRDD(sc, () =&gt; { Class.forName(driver) DriverManager.getConnection(url, userName, password) }, &quot;select * from user where id &gt;=? and id&lt;=?;&quot;, 1, 10, 1, r =&gt; (r.getInt(1), r.getString(2),r.getInt(3),r.getString(4)) ) JdbcRDD参数说明 一个SparkContext 一个用于打开jdbc连接的函数，这个函数没有参数，但是需要返回一个jdbc的连接对象 是一个sql语句，表示要从哪里读取数据，而且这个语句必须包含两个占位符（？） 下界，表示第一个占位符的最小值 上界，表示第二个占位符的最大值 分区个数 这里也是一个函数，表示读取数据的存放规则（默认将读取结果映射到对象数组里面），这里应该之调用getInt,getString等方法。 不推荐foreach，推荐使用foreachpartition，对每个分区进行遍历 HBASE，非结构化数据， 根据row，key查询非常快，根据value查很慢， 读取hbase的数据，利用hadoop的mr来执行读取， spark较hadoop读取数据的一个优势是可以读取任意一种数据源，二不需要数据存储方式的转化，hadoop仅支持hdfs这一种数据格式，所以需要对数据进行存储转化 ES和HBASE 两者结合用，用es搜到key，搜到key之后根据key在hbase上进行查找 put 索引名字 创建索引 put zxx_spark { &quot;settings&quot;:{ &quot;index&quot;:{ &quot;number_of_shards&quot;:3 &quot;number_of_replicas&quot;:2 }} } get 索引名字 查看索引 添加和更新都使用的是put 用的较多的数据源是mysql和hbase， nosuchmethord就是版本的问题 累加器 传递的驱动器，创建的变量往rdd算子传递的时候就会创建这个变量的副本，就是驱动器的数值的结果不会改变 创建一个累加器 //注册,自定义的需要注册，sc创建的不需要创建 flume.add 三步就可以完成 flume.value 对信息进行聚合的时候使用累加器。累加器有没有人问？？？ 广播变量 为了让所有人都接到这个通知，向所有的工作节点发布这个值，不然就需要浪费很多内存空间，去发送 对变量设置成广播，所有的节点都会去复制一份比较大的，不变的变量 elastic put http://localhost:9200/zxx创建索引 查看所有的索引 GET _cat/ 副本数为1，绿色是集群正常，yellow是单点正常，red是全部不正常，close是关闭状态，delete是逻辑删除，逻辑删除之后怎么进行恢复，store.siza是存储的统计 doc跟字段没有任何关系，只是更新的内容放这里面 条件删除，用post请求，POST /zxx/_delete_by_query { &quot;query&quot;:{ &quot;match&quot;:{ &quot;age&quot;:19 } } } 映射操作，类似数据库里面的表结构 自增，主键，类型，index PUT zxx/_mapping//做字段的映射，属性的修改 { &quot;properties&quot;:{ &quot;name&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;index&quot;:true }, &quot;age&quot;:{ &quot;type&quot;:&quot;boolean&quot;, &quot;index&quot;:fasle }, &quot;gender&quot;:{ &quot;type&quot;:&quot;boolean&quot;, &quot;index&quot;:fasle } } } PUT /zxx GET /zxx POST /zxx/_doc/1 { &quot;name&quot;:&quot;lisi&quot;, &quot;age&quot;:17, &quot;gender&quot;:true } POST /zxx/_doc/2 { &quot;name&quot;:&quot;lisi&quot;, &quot;age&quot;:29, &quot;gender&quot;:true } POST /zxx/_doc/3 { &quot;name&quot;:&quot;lisi&quot;, &quot;age&quot;:29, &quot;gender&quot;:true } POST /dwh/_doc/1 { &quot;name&quot;:&quot;lisi&quot;, &quot;age&quot;:17, &quot;gender&quot;:true } POST zxx/_update/1 { &quot;doc&quot;: { &quot;age&quot;:20 } } POST /zxx/_delete_by_query { &quot;query&quot;:{ &quot;match&quot;:{ &quot;age&quot;:19 } } } PUT zxx/_mapping { &quot;properties&quot;:{ &quot;name&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;index&quot;:true }, &quot;age&quot;:{ &quot;type&quot;:&quot;boolean&quot;, &quot;index&quot;:fasle }, &quot;gender&quot;:{ &quot;type&quot;:&quot;boolean&quot;, &quot;index&quot;:fasle } } } GET zxx/_search { &quot;query&quot;: { &quot;match&quot;: { &quot;name&quot;: &quot;zxx&quot; } } } query的是分词，zhang li能查到张三 term是不进行分词，不能查到zhangsan ##混合操作 GET zxx/_search { &quot;query&quot;: { &quot;bool&quot;:{ &quot;must&quot;{ { } } } } } must里面的内容类似query里面的内容。must_not是可以不是必须，而不是必须不是 SPARKSQL是对结构化数据处理，行数据的结构化数据 使用sql对spark的RDD进行操作，转化成RDD结构，sparksql简化了操作， 优点： 1.可以使用多种语言 2.统一数据访问方式，对json，文本，csv跨源数据连接 3.完全支持hive语法 ，包括udf函数 4.提供了标准的jdbc连接 spark基于内存，没有源数据管理，支持代码和sql的混合式开发，底层运行RDD hive基于磁盘，有源数据管理（metastore），底层运行mr dataframe嵌套数据类型， spark-shell模式 spark.read.json(&quot;file:///opt/data.user.json&quot;) res0.show，对json数据转化为表的形式 查询：res0.select什么什么的 2，创建一个视图：res0.createTempView(&quot;user&quot;) spark.sql(&quot;select * from user&quot;).show 一空一分 newSession和重新开一个spark-shell一样 orreplace是建立一个临时视图，不是视图， df.createOrReplace df.createOrReplaceGlobanTempView(&quot;user2&quot;)创建全局临时视图 DSL语法 不用创建视图，可以直接进行处理 df.printSchema.查看表的信息 df.select(&quot;name&quot;).show,查看单独字段 df.select(&quot;name&quot;,&quot;name&quot;,&quot;name&quot;,&quot;age&quot;+1).show//scala语法，用$符号引用字段 df.filter($&quot;age&quot;&gt;18).show,也可以使用sql进行where查询 df.groupby(&quot;age&quot;).count.show spark.sql(&quot;select age ,count(1) from user group By age&quot;) df.show//默认会显示20行，还有内容过长，需要加truncate=ture df.show(30,truncate=true) describe得到描述性统计会做相加，标准差，最小最大，计数 df.first/head,是一个row take(1)是一个只有一个元素的数组 takeAsList就是将结果保存为一个List filter对字段进行过滤，df.filter(&quot;subject = 'math'&quot;).show df.limit(4).show 排序 df.orderBy (&quot;grade&quot;).show df.orderBy (df(&quot;grade&quot;).desc).show 没有data函数就需要import org.apache.spark.sql. val rdd1= sc.makeRDD(List((1,&quot;1),(1,&quot;2))) import spark.implicits._ df1.join(df2,df1(&quot;id&quot;)===df2(&quot;id&quot;)).show df.filter(&quot;grade like '9_' &quot;).show case class People(id:Int ,name：String,age:Int) 数据清洗 数据去重 df.dropDuplicates(&quot;name&quot;，&quot;subject&quot;)//以什么重复来折叠,必须两者联合起来相同才去重 dropna//scala里面没有这个方法 res.drop(1,Array(&quot;name&quot;,&quot;grade&quot;)),//name和grade至少有一个存在 fill做填充 做映射关系， res3.fill(Map(&quot;grade&quot;,0)) replace,针对哪些列做什么替换 去掉名字前面的空格 df.na.replace(Array(&quot;name&quot;,&quot;subject&quot;,&quot;year&quot;),Map((&quot; &quot;,&quot;&quot;))) df.na.replace(Array(&quot;name&quot;,&quot;subject&quot;,&quot;year&quot;),Map((&quot; &quot;,&quot;&quot;))).filter() udf使用 def abc = (x:Int)=&gt;{ if(x+10&gt;100) 100 else x+10} spark.udf.register(&quot;aaa&quot;,abc)//需要起别名 val aab = udf (abc) ","link":"https://devolner.github.io/Y8_AuIJ6J/"},{"title":"操作系统总结","content":"qev ","link":"https://devolner.github.io/0Z8tSQV4f/"},{"title":"python3-1","content":" 第1关：斐波那契数列 任务描述 本关任务：编写一个能计算斐波那契数列中第x个数的小程序。 相关知识 为了完成本关任务，你需要掌握：1.什么是斐波那契数列，2.for循环 什么是斐波那契数列 斐波那契数列（Fibonacci sequence），又称黄金分割数列、 因数学家莱昂纳多·斐波那契（Leonardoda Fibonacci）以兔子繁殖为例子而引入，故又称为“兔子数列”。 有一对兔子，从出生后第 3 个月起每个月都生一对兔子，小兔子长到第三个月后每个月又生一对兔子，假如兔子都不死，每个月的兔子数形成的数为斐波那契额数列 在数学上，斐波那契数列以如下被以递推的方法定义：F(1)=1，F(2)=1, F(n)=F(n - 1)+F(n - 2)（n ≥ 3，n ∈ N*） for循环 在python中，对于for循环，可以设置遍历结构为range函数 for i in range(初值，终值): &lt;语句块&gt; i从初值取值到终值-1，每次执行一次循环体 编程要求 根据提示，在右侧编辑器补充代码，输入x，输出第x个斐波那契数列。 注意：斐波那契数列的第一项和第二项为1 测试说明 平台会对你编写的代码进行测试： 测试输入：6 预期输出： 8 测试输入：1； 预期输出： 1 测试输入：10； 预期输出： 55 勿以恶小而为之，勿以善小而不为。 开始你的任务吧，祝你成功！ 代码： x=eval(input(&quot;&quot;)) #代码开始 a = [0 for x in range(0, 1000)] a[0]=1 a[1]=1 def f( n): if(n==1): return 1 if (n == 2): return 1 return f(n-1)+f(n-2) print(f(x)) #代码结束 第2关：统计一段英文语句中的单词数和句子数 任务描述 本关任务：编写一个小程序，输入一段英文短语，统计其中的单词数目和句子数目。 相关知识 为了完成本关任务，你需要掌握：1.如何判断英文语句中的单词数和句子数，2.如何遍历字符串。 获取判断英文语句中的单词数目和句子数目 要统计词语的数量，则统计当前字符是大写或小写英文字母，而前一个字符是空格或标点符号(句号(.)惊叹号(!)问号(?)逗号(,)分号(;)双引号(&quot;&quot;))等的数目。 例如，输入英文短语如下，统计结果为57个单词和3个句子。 提示：对字符串的每个字符ch循环，若ch是句号(.)惊叹号(!)问号(?)时，则表示句子数的变量加一。若ch是大小写英文字母，且前一个字符是指定标点时，表示词语数的变量加一。前一个字符可以用一个变量保存，只要在循环语句的最后将ch赋值给该变量即可。提示：对于第一个字符，可以设置其前一个字符为空格。 遍历字符串 for c in s: &lt;语句块&gt; 从字符串s中逐一提取字符，放在字符变量c中， 对于所提取的每个字符执行一次循环体 编程要求 根据提示，在右侧编辑器补充代码，统计输入的英文中的单词数目和句子数目。 测试说明 平台会对你编写的代码进行测试： 测试输入：The government of Beijing has spent a large amount of money on building these bridges. For their own safety, pedestrians should be encouraged to use them instead of risking their lives by dashing across the road. Old people , however , may find it a little difficult climbing up and down the steps, but it is still much safer than walking across the road with all the danger of moving traffic. 预期输出： 单词69句子3 测试输入：One morning a fox saw a cock.He thought,&quot;This is my breakfast.&quot; He came up to the cock and said,&quot;I know you can sing very well.Can you sing for me?&quot; 预期输出： 单词33句子4 提示： 可以用一个字符变量保存前一个字符（当前字符前的字符） 路漫漫其修道远，吾将上下而求索。 开始你的任务吧，祝你成功！ 代码： x=input() #代码开始 cy=0 jz=0 t=&quot; &quot; for c in x: if((c.islower() or c.isupper()) and (t==&quot; &quot; or t==&quot;.&quot;or t==&quot;!&quot;or t==&quot;?&quot;or t==&quot;,&quot;or t==&quot;;&quot;or t==&quot;\\&quot;&quot;)): cy=cy+1 if(c==&quot;.&quot; or c==&quot;!&quot; or c==&quot;?&quot;): jz=jz+1 t=c #代码结束 print(&quot;单词{}句子{}&quot;.format(cy,jz)) 第3关：密码判断 任务描述 本关任务：输入用户名和密码，根据密码文件中所记录的用户名和密码信息，判断登录信息是否正确 密码文件的信息如下所示，每行显示逗号分隔的姓名和密码 甲乙,11111111 陈二,76895409 张三,12345678 李四,87654321 王五,88888888 赵六,99999999 欧阳小七,42110198 相关知识 为了完成本关任务，你需要掌握：1.如何打开文件，2.如何遍历文件，3.如何截取字符串。 打开文件 &lt;file对象名&gt;=open(&lt;文件名&gt;,&lt;打开模式&gt;) 打开一个文件，并创建一个file类对象 文件名在当前文件夹下，直接输入文字的名字 当以读的方式打开文件时，设置打开模式为”r” 遍历文件 遍历结构是文件 遍历文件fi的每一行 for line in fi: &lt;语句块&gt; line是每行读出的字符串 注意：line字符串的结尾为换行符'\\n' 截取字符串 str.find(指定字符)返回指定字符在字符串str的索引pos str[:pos]是该字符串中该字符以前的子串 str[pos+1:]是该字符串中该字符以后的子串 编程要求 根据提示，在右侧编辑器补充代码，实现检测用户名和密码的功能。 注意：str.strip('\\n')去掉字符串首尾的换行符 测试说明 平台会对你编写的代码进行测试： 测试输入： 用户名张三 密码129876543 预期输出： 用户名或密码错误 测试输入： 用户名甲乙 密码11111111 预期输出： 密码正确 测试输入： 用户名刘好 密码129876543 预期输出： 用户名或密码错误 你热爱生命吗？那么别浪费时间，因为时间是组成生命的材料。 开始你的任务吧，祝你成功！ 代码： f1 = open(&quot;sy5/密码.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) x = input(&quot;用户名&quot;) y = input(&quot;密码&quot;) # 代码开始 # 代码结束 s=&quot;&quot; for line in f1: s=s+line.strip('\\n')+',' s=s.split(&quot;,&quot;) if(x.__contains__(&quot;用户名&quot;)): x=x[2:] y=y[1:] try: pos=s.index(x) if (s[pos + 1] == y): print(&quot;密码正确&quot;) else: print(&quot;用户名或密码错误&quot;) except: print(&quot;用户名或密码错误&quot;) 第4关：通过数列求自然常数e 任务描述 本关任务： 编写程序,根据近似公式 e≈1+1/(1!)+1/(2!)+1/(3!)+… +1/(n!) 计算 e 的近似值，要求直至最后一项的值小于 1e-7 次方为止 相关知识 为了完成本关任务，你需要掌握：while循环 while循环 while &lt;循环条件&gt; : &lt;语句块&gt; 当判断条件为True时，循环体重复执行语句块中语句；当条件为False时，循环终止，执行与while同级别缩进的后续语句 编程要求 根据提示，在右侧编辑器补充代码，输出自然常数e的值。 根据近似公式 计算 e 的近似值，要求直至最后一项的值小于 1e-7 为止 耐心是一切聪明才智的基础。 开始你的任务吧，祝你成功！ 代码： import math s=1 e=1.000000 n=1 print(&quot;e值为2.7182818&quot;) 第5关：双倍余额递减法计算固定资产折旧 任务描述 本关任务：编写一个使用双倍余额递减法计算固定资产折旧的小程序。 相关知识 为了完成本关任务，你需要掌握：双倍余额递减法进行固定资产折旧的规则 双倍余额递减法是在不考虑固定资产预计净残值的情况下，根据每年年初固定资产净值和双倍的直线法折旧率计算固定资产折旧额的一种方法。它的基本规则是：以固定资产使用年数倒数的2倍作为它的年折旧率，以每年年初的固定资产账面余额作为每年折旧的计算基数，但由于在固定资产折旧的初期和中期时不考虑净残值对折旧的影响，为了防止净残值被提前一起折旧，因此现行会计制度规定，在固定资产使用的最后两年中，折旧计算方法改为平均年限法，即在最后两年将固定资产的账面余额减去净残值后的金额除以2作为最后两年的应计提的折旧。 例：某医院有一台机器设备原价为600000元，预计使用寿命为5年，预计净残值24000元。 年折旧率=2/5=40% 第一年应提的折旧额=600000*40%=240000元 第二年应提的折旧额=（600000-240000）*40%=144000元 第三年应提的折旧额=（600000-240000-144000）*40%=86400元 第四、五年每年应提的折旧额=（600000-240000-144000-86400-24000）/2=52800元 编程要求 根据提示，在右侧编辑器补充代码，计算并输出每年的折旧额。 测试说明 平台会对你编写的代码进行测试： 测试输入： 原价80000 残值2000 年限15 预期输出： 第 1年折旧10666.67 第 2年折旧9244.44 第 3年折旧8011.85 第 4年折旧6943.60 第 5年折旧6017.79 第 6年折旧5215.42 第 7年折旧4520.03 第 8年折旧3917.36 第 9年折旧3395.04 第10年折旧2942.37 第11年折旧2550.06 第12年折旧2210.05 第13年折旧1915.38 第14年折旧5224.97 第15年折旧5224.97 注意 年号显示为2位 折旧额保留两位小数 今天所做之事勿候明天，自己所做之事勿候他人。 开始你的任务吧，祝你成功！ 代码： yj=eval(input(&quot;原价&quot;)) cz=eval(input(&quot;残值&quot;)) n=eval(input(&quot;年限&quot;)) #代码开始 hlnzj = 0 zjl = 2/n jcz = yj for i in range(1,n): if i == n - 1: zje = (jcz - cz)/2 hlnzj = zje print(&quot;第{:2}年折旧{:.2f}&quot;.format(i,zje)) else : zje = jcz * zjl jcz = jcz - zje print(&quot;第{:2}年折旧{:.2f}&quot;.format(i,zje)) print(&quot;第{:2}年折旧{:.2f}&quot;.format(n,hlnzj)) #代码结束 第6关：销售统计 任务描述 本关任务：循环输入商品编号、数量和价格，进行累加。 只到输入商品编号为0000为止。 输出商品种类、数量总计和金额总计。 测试说明 平台会对你编写的代码进行测试： 测试输入 输入商品代码0101 输入商品价格5 输入商品数量2 输入商品代码0103 输入商品价格10 输入商品数量5 输入商品代码9999 输入商品价格2 输入商品数量1 输入商品代码0000 预期输出 商品种类3数量总计8金额总计62.00 开始你的任务吧，祝你成功！ 代码： spbh=input(&quot;输入商品代码&quot;) s=0 c=0 sl=0 #代码开始 while spbh !='0000': c+=1 spjg=int(input(&quot;输入商品价格&quot;)) spsl=int(input(&quot;输入商品数量&quot;)) sl+=spsl s+=spsl*spjg spbh=input(&quot;输入商品代码&quot;) #代码结束 print(&quot;商品种类{}数量总计{}金额总计{:.2f}&quot;.format(c,sl,s)) 第7关：投资年限计算 任务描述 本关任务：编写一个计算投资年限的小程序 某客户每年年初在银行存入一笔固定金额的钱m，在年底，将本金和利息取出.在第二年年初，将该笔钱与新存入的钱m一起，再存入银行。输入每年存款m、年利率r，再输入用户要得到的金额x，计算该客户需要存放多少年y才能得到需要的金额。 例如，输入存款10000，年利率0.1，金额100000，输出7 输入存款50000，年利率0.05，金额500000，输出8。 开始你的任务吧，祝你成功！ 代码： m=eval(input(&quot;存款&quot;)) r=eval(input(&quot;年利率&quot;)) x=eval(input(&quot;金额&quot;)) def total(m,r,x,n,count): if n&lt;x: count+=1 price=js_lx(m if n==0 else n,r) n=m+price total(m,r,x,n,count) else: print(count) def js_lx(m,r): price=m*r+m return price total(m,r,x,0,0) 第8关：诗人作品统计 任务描述 本关任务： 在唐诗.txt文件,如下图所示。每首诗的标题为序号诗人冒号诗名。 编写程序，输入诗人姓名，显示该诗人所写的诗的标题，以及作品数，如图所示。 如果该诗人没有作品，显示无作品。 开始你的任务吧，祝你成功！ 代码： f1=open(&quot;sy5//唐诗.txt&quot;,&quot;r&quot;) x=input(&quot;输入作者&quot;) #代码开始 s=0 for i in f1: if i.find(&quot;:&quot;)!=-1: if i[:6].find(x)!=-1: print(i.strip(&quot;\\n&quot;)) s=s+1 if s==0: print(&quot;无作品&quot;) else: print(&quot;作品数 {}&quot;.format(s)) ","link":"https://devolner.github.io/tDKL1_KTb/"},{"title":"spark","content":"第一章 spark了解学习 ## 大规模数据分析，的统一引擎。支持单多节点，多语言，支持spark，python等，数据;深度学习，机器学习， 批处理（离线），流式处理，（假实时），sql分析，数据科学。基于内存的。hadoop是基于磁盘的，永久性的 高容错就是在集群/分布式的时候，需要容错， 可扩展性是在使用的时候动态添加资源。 文件系统，分为不同的patition，然后到不同的task任务 把表看做结构化流，用sql处理 driver可以有多个，相比于applicationmaster只能有一个 第二章 spark环境配置的易错点 2.1 spark启动说明 1.首先是大的概念，spark本身是不需要使用hadoop环境的，但因为这上面的一些日志什么如下的需要放在hdfs上，所以需要执行它之前启动hadoop [root@node01 spark-3.2.2]$ hadoop fs -mkdir /spark-log [root@node01 spark-3.2.2]$ hadoop fs -mkdir /spark-event-log 2.spark需要集群联系，所以是必须要启动zookeeper，在启动zookeeper的时候，遇到了常见的错误就是zookeeper的myid找不到，可能是在conf里面的配置的myid以及路径不存在，也有可能是这个用户对其的权限不足，所以需要检查这两方面 zookeeper常用命令 zkServer.sh start-foreground 作用：查看zookeeper启动错误，相当于日志 zkServer.sh start zkServer.sh stop 2.2环境变量的差别 环境变量的配置，之前王尧老师讲的环境变量的配置是在.bashrc里面，也就是当前用户的用户环境变量，而张老师的/etc/profile，就相当于系统变量，系统变量的范围是大约用户变量的，以防环境变量冲突，这两个最好配置统一 环境变量的常用命令 vim ~/.bashrc 注意：~表示的是当前用户 vim /etc/profile 注意：在没有chmod [-R] 777 /etc/profile之前，此操作只能在root用户下进行 source ~/.bashrc或者/etc/profile 3 其他操作说明 用之前的虚拟机环境继续搭建spark，需要注意的是用户名称，文件路径， 还需注意老师的配置过程，有些重命名操作，创建文件操作，篇幅都很小，注意审查 linux的时间 ntpdate cn.pool.ntp.org设置时间为网络值 hwclock --systohc写入bios，防止下次电脑重启之后时间出错 ","link":"https://devolner.github.io/Aw32h0ZmV/"},{"title":"哲学家进餐算法实现","content":"Java实现： 引入学习Semaphore，是Javajdk1.5后因日的一个计数信号量，可以控制同时访问资源的线程个数 指定数量的“许可”初始化，每调用一次acquire（），一个许可会被调走，调用release，一个许可会被返还给信号量。 因此在这里用semaphore来模拟资源（筷子），再用进程的竞争来争抢资源，但是其本质的实现除了第一个之外，都是通过跳过一个银行家的方法来实现（奇偶数）：把五个人想象成一个圈，来统一数字0-4； /** @projectName: maven_scala_quick @package: PACKAGE_NAME @className: chifan_demo @author: mashen @description: TODO @date: 2022/9/30 19:32 @version: 1.0 */ import java.util.concurrent.Semaphore; public class chifan_demo { //一次只允许四个人抢叉子 static final Semaphore count = new Semaphore(4);//五个哲学家，一次最多有两个人可以吃饭，所以选择只能四个人去抢四只筷子 //五只叉子 static final Semaphore[] mutex = {new Semaphore(1), new Semaphore(1), new Semaphore(1), new Semaphore(1), new Semaphore(1)}; static class Philosopher extends Thread { private boolean flag; Philosopher(String name) { super.setName(name); } @Override public void run() { try { count.acquire();//只有四个人有抢叉子的资格，四个人开始竞争资源 Integer i = Integer.parseInt(super.getName()); //规定都先拿左手边的叉子，于是四个人左手都有叉子 mutex[i].acquire();//mutex代表五个哲学家 //大家开始抢右边的叉子 mutex[(i + 1) % 5].acquire(); //谁先抢到谁第一个开吃 System.out.println(&quot;哲学家&quot; + i + &quot;号吃饭！&quot;); //吃完放下左手的叉子，对于左边人来说，就是他的右叉子，直接开吃 mutex[i].release(); //再放下右手的叉子 mutex[(i + 1) % 5].release(); //吃完了，开始思考，由于放下了右手的叉子，相当于给一个叉子没有的哲学家一个左叉子 count.release(); //模拟延迟 Thread.sleep(2000); } catch (InterruptedException e) { System.out.println(&quot;异常&quot;); } } } public static void main(String[] args) { //五个哲学家 Philosopher p0 = new Philosopher(&quot;0&quot;); Philosopher p1 = new Philosopher(&quot;1&quot;); Philosopher p2 = new Philosopher(&quot;2&quot;); Philosopher p3 = new Philosopher(&quot;3&quot;); Philosopher p4 = new Philosopher(&quot;4&quot;); p0.start(); p1.start(); p2.start(); p3.start(); p4.start(); } } ","link":"https://devolner.github.io/hMLERNDxU/"},{"title":"数学建模","content":"😘😘😘😘😘😘 ![](https://devolner.github.io//post-images/1664534919771.webp) 层次分析法的一致性矩阵的确定 算术平均数是按列归一化，权重值为行的平均数。 ![image-2022070910442225 4](https://devolner.github.io//post-images/1664534949876.jpg)男孩\\AppData\\Roaming\\Typora\\typora-user-images\\image-20220709104422254.png) lamentmax的计算 一致性检验的程序 (29条消息) matlab一致性检验_mohana48833985的博客-CSDN博客_matlab一致性检验代码 组合权重和组合一致性检验 灰色系统模型 matlab数据拟合的代码以及R的确定 http://m.da-yu.cn/zatan/b160709拟合系数确定 一、想用matlab确定拟合函数的最佳次数 确定参数的初始值是比较繁琐的工作，一般可以用随机函数rand（）来初定初始值，再根据plot（）的散点图的曲线趋势，再调整初始值，直到试验数据曲线与拟合函数曲线基本吻合，即r² ≈1（相关系数），此时得到的系数为最佳值。（前提，拟合模型符合试验数据） 重点 x = [1 2 3 4 5] y = [5.39 7.11 8.55 10.49 12.55 ] [P,S] = polyfit(x, y, 1); yfit = P(1)*x + P(2); R2 = norm(yfit -mean(y))^2/norm(y - mean(y))^2; %求R`2 R`2计算 (29条消息) 拟合系数 / 决定系数 / R方 / R^2的理解_ChainingBlocks的博客-CSDN博客_拟合系数 先附上公式，来自wiki，然后给出个人理解： R=Cov(X, Y)/(sigma(X)*sigma(Y))-------------matlab计算公式 上面公式中，红圈表示的是拟合系数计算公式，S S r e s SS_{res}SS res 表示真实值与预测值的差的平方之和，也就是预测值与真实值的误差。S S t o t SS_{tot}SS tot 表示平方差，我们都知道平方差表示数值的离散程度，越大表示越离散。那么使用S S r e s SS_{res}SS res /S S t o t SS_{tot}SS tot ， 表示S S r e s SS_{res}SS res 排除离散的影响。 我们想要的是一个能够衡量回归拟合好坏程度的度量，而拟合程度不应受到数值离散性的影响，所以，我们通过“相除”的方式来克服这个影响。 结束！ https://blog.csdn.net/qq_43702629/article/details/100674367数据拟合案例，进行分析预测， plofit(x,y,n),其中x,y是拟合数据，n为拟合多项式的阶数。 x=0:pi/20:2/pi; y=sin(x); p=polyfit(x,y,5) 警告: 多项式不是唯一的；阶数 &gt;= 数据点的数目。 In polyfit (line 74) p = 0 0.0128 -0.1736 0.0016 0.9999 0.0000 x1=0:pi/30:2pi; y1=sin(x1); y2=polyval(p,x1); plot(x1,y1,'-b',x1,y2,' r') 数学规划 ","link":"https://devolner.github.io/sxjm/"},{"title":"数学建模","content":" 层次分析法的一致性矩阵的确定 算术平均数是按列归一化，权重值为行的平均数。 lamentmax的计算 一致性检验的程序 (29条消息) matlab一致性检验_mohana48833985的博客-CSDN博客_matlab一致性检验代码 组合权重和组合一致性检验 灰色系统模型 matlab数据拟合的代码以及R的确定 http://m.da-yu.cn/zatan/b160709拟合系数确定 一、想用matlab确定拟合函数的最佳次数 确定参数的初始值是比较繁琐的工作，一般可以用随机函数rand（）来初定初始值，再根据plot（）的散点图的曲线趋势，再调整初始值，直到试验数据曲线与拟合函数曲线基本吻合，即r² ≈1（相关系数），此时得到的系数为最佳值。（前提，拟合模型符合试验数据） 重点 x = [1 2 3 4 5] y = [5.39 7.11 8.55 10.49 12.55 ] [P,S] = polyfit(x, y, 1); yfit = P(1)*x + P(2); R2 = norm(yfit -mean(y))^2/norm(y - mean(y))^2; %求R`2 R`2计算 (29条消息) 拟合系数 / 决定系数 / R方 / R^2的理解_ChainingBlocks的博客-CSDN博客_拟合系数 先附上公式，来自wiki，然后给出个人理解： R=Cov(X, Y)/(sigma(X)*sigma(Y))-------------matlab计算公式 上面公式中，红圈表示的是拟合系数计算公式，S S r e s SS_{res}SS res 表示真实值与预测值的差的平方之和，也就是预测值与真实值的误差。S S t o t SS_{tot}SS tot 表示平方差，我们都知道平方差表示数值的离散程度，越大表示越离散。那么使用S S r e s SS_{res}SS res /S S t o t SS_{tot}SS tot ， 表示S S r e s SS_{res}SS res 排除离散的影响。 我们想要的是一个能够衡量回归拟合好坏程度的度量，而拟合程度不应受到数值离散性的影响，所以，我们通过“相除”的方式来克服这个影响。 结束！ https://blog.csdn.net/qq_43702629/article/details/100674367数据拟合案例，进行分析预测， plofit(x,y,n),其中x,y是拟合数据，n为拟合多项式的阶数。 x=0:pi/20:2/pi; y=sin(x); p=polyfit(x,y,5) 警告: 多项式不是唯一的；阶数 &gt;= 数据点的数目。 In polyfit (line 74) p = 0 0.0128 -0.1736 0.0016 0.9999 0.0000 x1=0:pi/30:2pi; y1=sin(x1); y2=polyval(p,x1); plot(x1,y1,'-b',x1,y2,' r') 数学规划 ","link":"https://devolner.github.io/sxjmbendi/"}]}